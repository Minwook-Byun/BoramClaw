#!/usr/bin/env python3
"""
Deep Weekly Retrospective - 1ë§Œì ë¶„ëŸ‰ì˜ ê¹Šì´ ìˆëŠ” í”¼ë“œë°± íšŒê³ 

ê¸°ì¡´ comprehensive_weekly_retrospectiveëŠ” "ì ìˆ˜íŒ"
ì´ê±´ ì§„ì§œ "íšŒê³ " - êµ¬ì²´ì  ì‚¬ë¡€, íŒ¨í„´, í”¼ë“œë°±, ì¡°ì–¸
"""

import json
import sys
import subprocess
import re
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, List
from collections import Counter

__version__ = "1.0.0"

TOOL_SPEC = {
    "name": "deep_weekly_retrospective",
    "description": "1ë§Œì ë¶„ëŸ‰ì˜ ê¹Šì´ ìˆëŠ” í”¼ë“œë°± íšŒê³  (Karpathy + Bitter Lesson)",
    "version": "1.0.0",
    "input_schema": {
        "type": "object",
        "properties": {
            "days_back": {
                "type": "integer",
                "description": "íšŒê³  ê¸°ê°„ (ì¼)",
                "default": 7
            }
        }
    }
}


def collect_git_commits(days_back: int, workdir: str) -> List[Dict[str, Any]]:
    """Git ì»¤ë°‹ ìˆ˜ì§‘"""
    commits = []
    since = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")

    try:
        result = subprocess.run(
            ["git", "log", f"--since={since}", "--pretty=format:%H|%ad|%s|%an", "--date=iso"],
            cwd=workdir,
            capture_output=True,
            text=True,
            timeout=10
        )

        for line in result.stdout.strip().split('\n'):
            if line:
                parts = line.split('|', 3)
                if len(parts) == 4:
                    commits.append({
                        "hash": parts[0][:7],
                        "date": parts[1][:10],
                        "time": parts[1][11:19],
                        "message": parts[2],
                        "author": parts[3]
                    })
    except Exception:
        pass

    return commits


def _parse_prompt_datetime(prompt: Dict[str, Any]) -> datetime | None:
    date_text = str(prompt.get("date", "")).strip()
    if not date_text:
        return None
    time_text = str(prompt.get("time", "")).strip() or "00:00:00"
    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d"):
        try:
            if fmt == "%Y-%m-%d":
                return datetime.strptime(date_text, fmt)
            return datetime.strptime(f"{date_text} {time_text}", fmt)
        except ValueError:
            continue
    return None


def _dedupe_prompts(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    deduped = []
    for row in items:
        key = (
            row.get("source", ""),
            row.get("date", ""),
            row.get("time", ""),
            row.get("content", ""),
        )
        if key in seen:
            continue
        seen.add(key)
        deduped.append(row)
    return deduped


def _normalize_prompt_text(raw: Any) -> str:
    return " ".join(str(raw or "").split()).strip()


def _prompt_quality_score(prompt: Dict[str, Any]) -> float:
    """í”„ë¡¬í”„íŠ¸ ë‚´ìš© í’ˆì§ˆ ì ìˆ˜ (0-100)."""
    text = _normalize_prompt_text(prompt.get("full_content", "") or prompt.get("content", ""))
    if not text:
        return 0.0
    lower = text.lower()
    length = len(text)
    score = 40.0

    # ê¸¸ì´ í’ˆì§ˆ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸´ í”„ë¡¬í”„íŠ¸ íŒ¨ë„í‹°)
    if length < 8:
        score -= 30
    elif length < 20:
        score -= 14
    elif 25 <= length <= 220:
        score += 18
    elif 221 <= length <= 380:
        score += 9
    elif length > 500:
        score -= 8

    # ë§¥ë½/ëª©í‘œ/ì œì•½ ì‹ í˜¸
    context_tokens = ("ì™œ", "ì´ìœ ", "ë°°ê²½", "ë¬¸ì œ", "ì›ì¸", "ë§‰í˜€", "context", "because", "error", "ì—ëŸ¬")
    goal_tokens = ("ëª©í‘œ", "ì„±ê³µ", "ì™„ë£Œ", "í†µê³¼", "ê²€ì¦", "ê¸°ì¤€", "done", "pass", "success", "acceptance")
    constraint_tokens = ("ìµœì†Œ", "ìµœëŒ€", "ì œì•½", "ì œí•œ", "ì‹œê°„", "ì„±ëŠ¥", "ë³´ì•ˆ", "days_back", "deadline", "timeout")
    if any(token in lower for token in context_tokens):
        score += 9
    if any(token in lower for token in goal_tokens):
        score += 10
    if any(token in lower for token in constraint_tokens):
        score += 7

    # êµ¬ì²´ì„± ì‹ í˜¸ (ìˆ«ì/íŒŒì¼/ê²½ë¡œ/ì§ˆë¬¸)
    number_hits = len(re.findall(r"\b\d+\b", text))
    score += min(number_hits * 1.5, 8.0)
    path_hits = len(re.findall(r"(?:/[A-Za-z0-9._-]+)+|\b[A-Za-z0-9._-]+\.[A-Za-z0-9]{1,8}\b", text))
    score += min(path_hits * 2.0, 8.0)
    if "?" in text:
        score += 5

    # ë©€í‹° ì¸í…íŠ¸ ê³¼ë‹¤/ë…¸ì´ì¦ˆ íŒ¨ë„í‹°
    multi_intent_tokens = (" ê·¸ë¦¬ê³  ", " ë˜ ", " ê·¸ë¦¬ê³ ë‚˜ì„œ ", " then ", " also ", " additionally ")
    multi_intent_count = sum(lower.count(tok.strip()) for tok in multi_intent_tokens)
    if multi_intent_count >= 4 and length > 220:
        score -= 6

    noise_markers = (
        "context from my ide setup",
        "## active file:",
        "## open tabs:",
        "[request interrupted",
    )
    if any(marker in lower for marker in noise_markers):
        score -= 28

    return max(0.0, min(score, 100.0))


def _prompt_fingerprint(prompt: Dict[str, Any]) -> str:
    text = _normalize_prompt_text(prompt.get("content", "")).lower()
    compact = re.sub(r"[^0-9a-zA-Zê°€-í£ ]+", " ", text)
    compact = re.sub(r"\s+", " ", compact).strip()
    return compact[:140]


def _pick_quality_examples(items: List[Dict[str, Any]], limit: int, strategy: str = "high") -> List[Dict[str, Any]]:
    """
    í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì˜ˆì‹œ ì„ íƒ.
    strategy:
    - high: ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ìš°ì„ 
    - low: ê°œì„  í•„ìš” í”„ë¡¬í”„íŠ¸ ìš°ì„ 
    """
    if limit <= 0 or not items:
        return []

    scored = []
    for idx, item in enumerate(items):
        quality = _prompt_quality_score(item)
        ts = _parse_prompt_datetime(item) or datetime.min
        scored.append((quality, ts, idx, item))

    reverse = strategy != "low"
    scored.sort(key=lambda x: (x[0], x[1], -x[2]), reverse=reverse)

    chosen: List[Dict[str, Any]] = []
    seen_fp = set()
    source_quota: Dict[str, int] = {}
    date_quota: Dict[str, int] = {}
    max_per_source = max(1, (limit + 1) // 2)
    max_per_date = max(1, (limit + 1) // 2)

    for quality, _ts, _idx, item in scored:
        fp = _prompt_fingerprint(item)
        if not fp or fp in seen_fp:
            continue
        source = str(item.get("source", "unknown"))
        date_label = str(item.get("date", "unknown"))
        if source_quota.get(source, 0) >= max_per_source:
            continue
        if date_quota.get(date_label, 0) >= max_per_date:
            continue
        row = dict(item)
        row["_quality_score"] = round(quality, 1)
        chosen.append(row)
        seen_fp.add(fp)
        source_quota[source] = source_quota.get(source, 0) + 1
        date_quota[date_label] = date_quota.get(date_label, 0) + 1
        if len(chosen) >= limit:
            break

    # quota ë•Œë¬¸ì— ë¶€ì¡±í•  ê²½ìš° ì™„í™”
    if len(chosen) < limit:
        for quality, _ts, _idx, item in scored:
            fp = _prompt_fingerprint(item)
            if not fp or fp in seen_fp:
                continue
            row = dict(item)
            row["_quality_score"] = round(quality, 1)
            chosen.append(row)
            seen_fp.add(fp)
            if len(chosen) >= limit:
                break

    return chosen


def collect_prompt_windows(days_back: int, workdir: str) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]], Dict[str, Any]]:
    """
    ìµœê·¼ Nì¼(current) + ê·¸ ì´ì „ Nì¼(previous) í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì§‘.
    universal_prompt_collectorë¥¼ ì‹¤í–‰í•´ ìµœì‹  ë°ì´í„°ë¥¼ ì¬ìƒì„±í•œë‹¤.
    """
    meta: Dict[str, Any] = {"collector_success": False}
    all_prompts: List[Dict[str, Any]] = []

    try:
        from universal_prompt_collector import run as run_universal_prompt_collector

        collect_days = max(days_back * 2, 14)
        collector_result = run_universal_prompt_collector(
            {
                "days_back": collect_days,
                "sources": ["all"],
                "min_length": 5,
            },
            {"workdir": workdir},
        )
        if isinstance(collector_result, dict):
            meta["collector_success"] = bool(collector_result.get("success"))
            meta["collector_output_file"] = collector_result.get("output_file", "")
            meta["collector_by_source"] = collector_result.get("by_source", {})

            output_file = collector_result.get("output_file", "")
            if isinstance(output_file, str) and output_file:
                out_path = Path(output_file)
                if out_path.exists():
                    with open(out_path, "r", encoding="utf-8") as f:
                        for line in f:
                            try:
                                row = json.loads(line)
                            except json.JSONDecodeError:
                                continue
                            if isinstance(row, dict):
                                source = str(row.get("source", ""))
                                if source in {"codex_session", "codex"}:
                                    row["source"] = "codex"
                                all_prompts.append(row)
    except Exception as exc:
        meta["collector_error"] = str(exc)

    # fallback: ê¸°ì¡´ íŒŒì¼ (ì˜¤ëŠ˜) ë¡œë“œ
    if not all_prompts:
        today = datetime.now().strftime("%Y%m%d")
        fallback_file = Path(workdir) / "logs" / f"prompts_collected_{today}.jsonl"
        meta["fallback_file"] = str(fallback_file)
        if fallback_file.exists():
            with open(fallback_file, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        row = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    if isinstance(row, dict):
                        source = str(row.get("source", ""))
                        if source in {"codex_session", "codex"}:
                            row["source"] = "codex"
                        all_prompts.append(row)

    all_prompts = _dedupe_prompts(all_prompts)

    now = datetime.now()
    current_cutoff = now - timedelta(days=days_back)
    previous_cutoff = now - timedelta(days=days_back * 2)

    current_prompts: List[Dict[str, Any]] = []
    previous_prompts: List[Dict[str, Any]] = []
    for row in all_prompts:
        ts = _parse_prompt_datetime(row)
        if ts is None:
            current_prompts.append(row)
            continue
        if ts >= current_cutoff:
            current_prompts.append(row)
        elif previous_cutoff <= ts < current_cutoff:
            previous_prompts.append(row)

    current_prompts = _dedupe_prompts(current_prompts)
    previous_prompts = _dedupe_prompts(previous_prompts)
    meta["total_loaded"] = len(all_prompts)
    meta["current_prompts"] = len(current_prompts)
    meta["previous_prompts"] = len(previous_prompts)
    return current_prompts, previous_prompts, meta


def deep_karpathy_analysis(prompts: List[Dict], commits: List[Dict]) -> str:
    """Karpathy ì›ì¹™ ê¹Šì´ ìˆëŠ” ë¶„ì„ (3000ì)"""
    lines = []
    lines.append("## ğŸ¯ Part 2: Karpathy ì›ì¹™ - ê¹Šì´ ìˆëŠ” ë¶„ì„")
    lines.append("")

    # 1. Think Before Coding
    lines.append("### 1. Think Before Coding: ê°€ì •í•˜ì§€ ë§ê³  ì§ˆë¬¸í•˜ë¼")
    lines.append("")

    question_prompts = [p for p in prompts if '?' in p.get('content', '') or any(
        word in p.get('content', '').lower()
        for word in ['ì–´ë–»ê²Œ', 'ì™œ', 'ë­', 'ë¬´ì—‡']
    )]
    command_prompts = [p for p in prompts if any(
        word in p.get('content', '') for word in ['í•´ì¤˜', 'ë§Œë“¤ì–´', 'ì¶”ê°€']
    )]

    q_ratio = len(question_prompts) / max(len(prompts), 1) * 100

    lines.append(f"**ì§ˆë¬¸í˜• í”„ë¡¬í”„íŠ¸**: {len(question_prompts)}ê°œ ({q_ratio:.1f}%)")
    lines.append(f"**ì§€ì‹œí˜• í”„ë¡¬í”„íŠ¸**: {len(command_prompts)}ê°œ ({len(command_prompts)/max(len(prompts),1)*100:.1f}%)")
    lines.append("")

    if question_prompts:
        lines.append("**ì¢‹ì€ ì§ˆë¬¸ ì‚¬ë¡€**:")
        for p in _pick_quality_examples(question_prompts, 3, strategy="high"):
            source = p.get('source', 'unknown')
            content = p.get('content', '')[:100]
            date = p.get('date', '')
            quality = p.get("_quality_score", 0)
            lines.append(f"- \"{content}\" ({source}, {date}, í’ˆì§ˆ {quality:.1f})")
        lines.append("")

    if command_prompts:
        lines.append("**ì§€ì‹œí˜• ì‚¬ë¡€** (ê°œì„  ê°€ëŠ¥):")
        for p in _pick_quality_examples(command_prompts, 3, strategy="low"):
            source = p.get('source', 'unknown')
            content = p.get('content', '')[:100]
            date = p.get('date', '')
            quality = p.get("_quality_score", 0)
            lines.append(f"- \"{content}\" ({source}, {date}, í’ˆì§ˆ {quality:.1f})")
        lines.append("")

    lines.append("**ë¶„ì„**:")
    if q_ratio < 30:
        lines.append("âš ï¸ ì§ˆë¬¸í˜• í”„ë¡¬í”„íŠ¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        lines.append("")
        lines.append("**ì™œ ë¬¸ì œì¸ê°€?**")
        lines.append("ì§€ì‹œí˜• í”„ë¡¬í”„íŠ¸ëŠ” 'í•´ê²°ì±…ì„ ê°€ì •'í•©ë‹ˆë‹¤.")
        lines.append("ì˜ˆ: \"ì»¤ë°‹ê³¼ í‘¸ì‹œí•´ì¤˜\" â†’ ì»¤ë°‹ì´ í•´ê²°ì±…ì´ë¼ê³  ê°€ì •")
        lines.append("")
        lines.append("í•˜ì§€ë§Œ ì§„ì§œ ë¬¸ì œëŠ”:")
        lines.append("- ì»¤ë°‹ ë©”ì‹œì§€ê°€ ë¶ˆëª…í™•í•œê°€?")
        lines.append("- ë³€ê²½ì‚¬í•­ì´ ë„ˆë¬´ ë§ì€ê°€?")
        lines.append("- í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í•˜ëŠ”ê°€?")
        lines.append("")
        lines.append("**ì§ˆë¬¸í˜•ìœ¼ë¡œ ë°”ê¾¸ë©´**:")
        lines.append("\"ì§€ê¸ˆ ì»¤ë°‹í•  ì¤€ë¹„ê°€ ëëŠ”ì§€ í™•ì¸í•´ì¤„ë˜? ë­ê°€ ë¹ ì¡ŒëŠ”ì§€ ì²´í¬í•´ë´\"")
        lines.append("")
        lines.append("**ë‹¤ìŒ ì£¼ ì‹¤í—˜**:")
        lines.append("í”„ë¡¬í”„íŠ¸ ì‘ì„± ì „ 3ì´ˆ ë©ˆì¶”ê³ :")
        lines.append("'ë‚´ê°€ í•´ê²°ì±…ì„ ê°€ì •í•˜ê³  ìˆëŠ”ê°€?' ìë¬¸í•˜ê¸°")
        lines.append("")
        lines.append("**ëª©í‘œ**: ì§ˆë¬¸í˜• í”„ë¡¬í”„íŠ¸ 50% ì´ìƒ")
    else:
        lines.append("âœ… ì§ˆë¬¸í˜• í”„ë¡¬í”„íŠ¸ ë¹„ìœ¨ì´ ì¢‹ìŠµë‹ˆë‹¤!")
        lines.append(f"{q_ratio:.1f}%ëŠ” ê±´ê°•í•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        lines.append("")
        lines.append("**ìœ ì§€ ë°©ë²•**:")
        lines.append("- í”„ë¡¬í”„íŠ¸ì— 'ì™œ', 'ì–´ë–»ê²Œ' í¬í•¨í•˜ê¸°")
        lines.append("- ê´€ì°° ë¨¼ì €, ì§€ì‹œëŠ” ë‚˜ì¤‘ì—")
        lines.append("- '~í•´ì¤˜' ëŒ€ì‹  '~ì–´ë–»ê²Œ í•˜ë©´ ë ê¹Œ?'")

    lines.append("")
    lines.append("---")
    lines.append("")

    # 2. Simplicity First
    lines.append("### 2. Simplicity First: ë‹¨ìˆœí•¨ì´ ìµœê³ ")
    lines.append("")

    refactor_commits = [c for c in commits if any(
        word in c['message'].lower()
        for word in ['ë¦¬íŒ©í† ë§', 'ë‹¨ìˆœí™”', 'ì •ë¦¬', 'refactor', 'simplify', 'clean']
    )]

    lines.append(f"**ë¦¬íŒ©í† ë§ ì»¤ë°‹**: {len(refactor_commits)}ê°œ / ì „ì²´ {len(commits)}ê°œ")
    lines.append("")

    if refactor_commits:
        lines.append("**ë‹¨ìˆœí™” ì‘ì—…**:")
        for c in refactor_commits:
            lines.append(f"- {c['date']}: {c['message']}")
        lines.append("")
        lines.append("âœ… ì½”ë“œ ë‹¨ìˆœí™”ë¥¼ ì˜ì‹í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
    else:
        lines.append("âš ï¸ ë¦¬íŒ©í† ë§ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
        lines.append("")
        lines.append("**Karpathyì˜ ì¡°ì–¸**:")
        lines.append("\"200ì¤„ì§œë¦¬ ì½”ë“œê°€ 50ì¤„ë¡œ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤ë©´ ë‹¤ì‹œ ì¨ë¼\"")
        lines.append("")
        lines.append("**ë³µì¡ë„ì˜ ì§•í›„**:")
        lines.append("- ê°™ì€ ì½”ë“œë¥¼ 3ë²ˆ ì´ìƒ ë³µë¶™")
        lines.append("- í•¨ìˆ˜ê°€ 50ì¤„ ë„˜ìŒ")
        lines.append("- if ì¤‘ì²©ì´ 3ë‹¨ê³„ ì´ìƒ")
        lines.append("- ë³€ìˆ˜ ì´ë¦„ì— ìˆ«ì (data1, data2...)")
        lines.append("")
        lines.append("**ë‹¤ìŒ ì£¼ ì•¡ì…˜**:")
        lines.append("1. ê°€ì¥ ê¸´ í•¨ìˆ˜ ì°¾ê¸°")
        lines.append("2. 3ê°œ ì´ìƒ ì‘ì€ í•¨ìˆ˜ë¡œ ë¶„ë¦¬")
        lines.append("3. ì»¤ë°‹ ë©”ì‹œì§€ì— 'refactor:' íƒœê·¸ ë¶™ì´ê¸°")

    lines.append("")
    lines.append("---")
    lines.append("")

    # 3. Surgical Changes
    lines.append("### 3. Surgical Changes: ìš”ì²­ëœ ê²ƒë§Œ ë³€ê²½")
    lines.append("")

    commit_dates = Counter(c['date'] for c in commits)
    if len(commit_dates) == 1 and commits:
        single_date = list(commit_dates.keys())[0]
        lines.append(f"âš ï¸ **ëª¨ë“  ì»¤ë°‹ì´ {single_date} í•˜ë£¨ì— ì§‘ì¤‘**")
        lines.append("")
        lines.append("**ì™œ ë¬¸ì œì¸ê°€?**")
        lines.append("í•˜ë£¨ì— ëª°ì•„ì„œ ì‘ì—…í•˜ë©´:")
        lines.append("- ì»¤ë°‹ ë‹¨ìœ„ê°€ ì»¤ì§")
        lines.append("- ì—¬ëŸ¬ ë³€ê²½ì´ ì„ì„")
        lines.append("- ë¡¤ë°±ì´ ì–´ë ¤ì›€")
        lines.append("- ë¦¬ë·°ê°€ í˜ë“¦")
        lines.append("")
        lines.append("**ì˜ˆì‹œ**:")
        for c in commits[:3]:
            lines.append(f"- {c['time']}: {c['message']}")
        lines.append("")
        lines.append("ì´ ì»¤ë°‹ë“¤ì´ ì •ë§ í•œ ë²ˆì— ì´ë£¨ì–´ì ¸ì•¼ í–ˆë‚˜ìš”?")
        lines.append("")
        lines.append("**ë‹¤ìŒ ì£¼ ì‹¤í—˜**:")
        lines.append("- ë§¤ì¼ ìµœì†Œ 1ì»¤ë°‹")
        lines.append("- í•œ ì»¤ë°‹ = í•œ ê°€ì§€ ë³€ê²½")
        lines.append("- í…ŒìŠ¤íŠ¸ â†’ ì»¤ë°‹ â†’ ë‹¤ìŒ ì‘ì—…")
    elif len(commits) > 0:
        avg_msg_len = sum(len(c['message']) for c in commits) / len(commits)
        lines.append(f"**í‰ê·  ì»¤ë°‹ ë©”ì‹œì§€ ê¸¸ì´**: {avg_msg_len:.1f}ì")
        lines.append("")
        if 20 <= avg_msg_len <= 80:
            lines.append("âœ… ì ì ˆí•œ ì»¤ë°‹ í¬ê¸°ì…ë‹ˆë‹¤!")
            lines.append("ì§§ì§€ë„, ê¸¸ì§€ë„ ì•Šì€ ë©”ì‹œì§€ = ì ì ˆí•œ ë²”ìœ„ì˜ ë³€ê²½")
        else:
            lines.append("âš ï¸ ì»¤ë°‹ í¬ê¸° ì¡°ì • í•„ìš”")

    lines.append("")
    lines.append("---")
    lines.append("")

    # 4. Goal-Driven
    lines.append("### 4. Goal-Driven: ê²€ì¦ ê°€ëŠ¥í•œ ëª©í‘œ")
    lines.append("")

    goal_keywords = ['í…ŒìŠ¤íŠ¸', 'ì™„ë£Œ', 'ì„±ê³µ', 'í†µê³¼', 'test', 'pass', 'done', 'âœ…']
    goal_prompts = [p for p in prompts if any(
        word in p.get('content', '').lower() for word in goal_keywords
    )]

    lines.append(f"**ëª©í‘œ ì§€í–¥ í”„ë¡¬í”„íŠ¸**: {len(goal_prompts)}ê°œ / {len(prompts)}ê°œ")
    lines.append("")

    if goal_prompts:
        lines.append("**ê²€ì¦ ê°€ëŠ¥í•œ ëª©í‘œ ì‚¬ë¡€**:")
        for p in _pick_quality_examples(goal_prompts, 3, strategy="high"):
            content = p.get('content', '')[:100]
            quality = p.get("_quality_score", 0)
            lines.append(f"- \"{content}\" (í’ˆì§ˆ {quality:.1f})")
        lines.append("")

    lines.append("**Karpathyì˜ ì¡°ì–¸**:")
    lines.append("âŒ ì•½í•œ ëª©í‘œ: \"ë²„ê·¸ ê³ ì³\"")
    lines.append("âœ… ê°•í•œ ëª©í‘œ: \"ì¬í˜„ í…ŒìŠ¤íŠ¸ ì‘ì„± â†’ í†µê³¼ì‹œí‚¤ê¸°\"")
    lines.append("")

    if len(goal_prompts) < len(prompts) * 0.2:
        lines.append("**ë¬¸ì œ**: ëŒ€ë¶€ë¶„ì˜ í”„ë¡¬í”„íŠ¸ê°€ ëª©í‘œë¥¼ ëª…ì‹œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        lines.append("")
        lines.append("**ê°œì„  ë°©ë²•**:")
        lines.append("1. ì‘ì—… ì‹œì‘ ì „: \"ì´ ì‘ì—…ì˜ ì™„ë£Œ ì¡°ê±´ì€?\"")
        lines.append("2. í”„ë¡¬í”„íŠ¸ì— í¬í•¨: \"~ê°€ ì™„ë£Œë˜ë©´ ì„±ê³µ\"")
        lines.append("3. ì»¤ë°‹ ì „: \"ëª©í‘œë¥¼ ë‹¬ì„±í–ˆëŠ”ê°€?\"")
        lines.append("")
        lines.append("**ì˜ˆì‹œ**:")
        lines.append("Before: \"ë¡œê·¸ì¸ ê¸°ëŠ¥ ë§Œë“¤ì–´ì¤˜\"")
        lines.append("After: \"ë¡œê·¸ì¸ ê¸°ëŠ¥ ë§Œë“¤ì–´ì¤˜. ì„±ê³µ ì¡°ê±´: í…ŒìŠ¤íŠ¸ 3ê°œ í†µê³¼\"")

    lines.append("")

    return "\n".join(lines)


def deep_bitter_lesson_analysis(prompts: List[Dict], prev_prompts: List[Dict]) -> str:
    """Bitter Lesson ê¹Šì´ ìˆëŠ” ë¶„ì„ (2000ì)"""
    lines = []
    lines.append("## ğŸ’¡ Part 3: Bitter Lesson - í’ˆì§ˆ vs ì–‘")
    lines.append("")
    lines.append("> \"ìŠ¤ì¼€ì¼ë˜ëŠ” í•™ìŠµ ì‹œìŠ¤í…œì´ ê²°êµ­ ì´ê¸´ë‹¤\"")
    lines.append("> \"ì˜ë¦¬í•¨ì€ 'ê¸°ëŠ¥ ì¶”ê°€'ê°€ ì•„ë‹ˆë¼ 'í•™ìŠµ ê°€ëŠ¥í•œ êµ¬ì¡° ì„¤ê³„'ì— ì¨ë¼\"")
    lines.append("")

    # í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ë¶„ì„
    lengths = [len(p.get('content', '')) for p in prompts]
    avg_length = sum(lengths) / len(lengths) if lengths else 0
    quality_scores = [_prompt_quality_score(p) for p in prompts]
    avg_quality = sum(quality_scores) / max(len(quality_scores), 1)
    good_quality = sum(1 for s in quality_scores if s >= 70)
    low_quality = sum(1 for s in quality_scores if s < 40)

    lines.append(f"**í‰ê·  í”„ë¡¬í”„íŠ¸ ê¸¸ì´**: {avg_length:.1f}ì")
    lines.append(f"**í‰ê·  í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ ì ìˆ˜(0-100)**: {avg_quality:.1f}")
    lines.append(f"- ìš°ìˆ˜(70+): {good_quality}ê°œ ({good_quality/max(len(prompts),1)*100:.1f}%)")
    lines.append(f"- ê°œì„  í•„ìš”(<40): {low_quality}ê°œ ({low_quality/max(len(prompts),1)*100:.1f}%)")
    lines.append("")

    # ê¸¸ì´ë³„ ë¶„ë¥˜
    short = [p for p in prompts if len(p.get('content', '')) < 30]
    medium = [p for p in prompts if 30 <= len(p.get('content', '')) <= 200]
    long = [p for p in prompts if len(p.get('content', '')) > 200]

    lines.append("**ê¸¸ì´ ë¶„í¬**:")
    prompt_count = max(len(prompts), 1)
    lines.append(f"- ì§§ìŒ (<30ì): {len(short)}ê°œ ({len(short)/prompt_count*100:.1f}%)")
    lines.append(f"- ì ì • (30-200ì): {len(medium)}ê°œ ({len(medium)/prompt_count*100:.1f}%)")
    lines.append(f"- ê¸º (>200ì): {len(long)}ê°œ ({len(long)/prompt_count*100:.1f}%)")
    lines.append("")

    if short:
        lines.append("**ë„ˆë¬´ ì§§ì€ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ**:")
        for p in _pick_quality_examples(short, 3, strategy="low"):
            content = p.get('content', '')
            quality = p.get("_quality_score", 0)
            lines.append(f"- \"{content}\" ({len(content)}ì, í’ˆì§ˆ {quality:.1f})")
        lines.append("")
        lines.append("**ë¬¸ì œ**: ë§¥ë½ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        lines.append("AIëŠ” ë‹¹ì‹ ì˜ ì˜ë„ë¥¼ ì¶”ì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.")
        lines.append("")
        lines.append("**ê°œì„  ì˜ˆì‹œ**:")
        lines.append("Before: \"ì»¤ë°‹í•´ì¤˜\" (7ì)")
        lines.append("After: \"ë³€ê²½ëœ íŒŒì¼ë“¤ í™•ì¸í•˜ê³ , ì˜ë¯¸ ìˆëŠ” ì»¤ë°‹ ë©”ì‹œì§€ë¡œ ì»¤ë°‹í•´ì¤˜\" (36ì)")
        lines.append("")

    if long:
        lines.append("**ê¸´ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ**:")
        for p in _pick_quality_examples(long, 2, strategy="high"):
            content = p.get('content', '')[:100]
            quality = p.get("_quality_score", 0)
            lines.append(f"- \"{content}...\" ({len(p.get('content', ''))}ì, í’ˆì§ˆ {quality:.1f})")
        lines.append("")
        lines.append("**ë¶„ì„**: ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ë‘ ê°€ì§€ ê°€ëŠ¥ì„±:")
        lines.append("1. âœ… ë§¥ë½ì´ í’ë¶€í•¨ (ì¢‹ìŒ)")
        lines.append("2. âš ï¸ ì—¬ëŸ¬ ìš”ì²­ì´ ì„ì„ (ë‚˜ì¨)")
        lines.append("")
        lines.append("**ì²´í¬**: ê¸´ í”„ë¡¬í”„íŠ¸ë¥¼ 2-3ê°œë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‚˜ìš”?")
        lines.append("ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤ë©´ â†’ ë‚˜ëˆ„ëŠ” ê²Œ ì¢‹ìŠµë‹ˆë‹¤")
        lines.append("")

    # ë°˜ë³µ íŒ¨í„´ ê°ì§€
    prompt_starts = [p.get('content', '')[:30].lower() for p in prompts]
    repeated = [(text, count) for text, count in Counter(prompt_starts).items() if count > 3]

    if repeated:
        lines.append("**ë°˜ë³µë˜ëŠ” í”„ë¡¬í”„íŠ¸ íŒ¨í„´** (ìë™í™” ê³ ë ¤):")
        for text, count in repeated[:3]:
            lines.append(f"- \"{text}...\" ({count}íšŒ)")
        lines.append("")
        lines.append("**Bitter Lesson ì ìš©**:")
        lines.append("ë°˜ë³µ = ìŠ¤ìºí´ë”© í•„ìš” ì‹ í˜¸")
        lines.append("")
        lines.append("**ìë™í™” ë°©ë²•**:")
        lines.append("1. ìŠ¤í¬ë¦½íŠ¸ë¡œ ë§Œë“¤ê¸°")
        lines.append("2. Git alias ì„¤ì •")
        lines.append("3. BoramClaw ë„êµ¬ë¡œ ë“±ë¡")
        lines.append("")

    # í’ˆì§ˆ ì¶”ì´
    if prev_prompts:
        prev_avg = sum(len(p.get('content', '')) for p in prev_prompts) / max(len(prev_prompts), 1)
        delta = avg_length - prev_avg
        lines.append(f"**ì „ì£¼ ëŒ€ë¹„**: {'+' if delta > 0 else ''}{delta:.1f}ì")
        lines.append("")
        if delta > 10:
            lines.append("ğŸ“ˆ í”„ë¡¬í”„íŠ¸ê°€ ë” ìƒì„¸í•´ì¡ŒìŠµë‹ˆë‹¤!")
        elif delta < -10:
            lines.append("ğŸ“‰ í”„ë¡¬í”„íŠ¸ê°€ ì§§ì•„ì¡ŒìŠµë‹ˆë‹¤.")
        else:
            lines.append("â¡ï¸ í‰ê·  ê¸¸ì´ ìœ ì§€")
        lines.append("")

    # Boris Chernyì˜ êµí›ˆ
    lines.append("**Boris Cherny (Claude Code ì°½ì‹œì)ì˜ êµí›ˆ**:")
    lines.append("")
    lines.append("1. **ë§Œë“¤ë˜ ì§‘ì°©í•˜ì§€ ë§ ê²ƒ**")
    lines.append("   - ì´ë²ˆ ì£¼ ë§Œë“  ê¸°ëŠ¥ë„ ë‹¤ìŒ ëª¨ë¸ì—ì„  ë¶ˆí•„ìš”í•  ìˆ˜ ìˆìŒ")
    lines.append("   - ìœ ì—°í•˜ê²Œ, ë²„ë¦´ ì¤€ë¹„ë¥¼ í•˜ê³ ")
    lines.append("")
    lines.append("2. **ì¸¡ì •í•˜ë˜ ê³¼ì‹ í•˜ì§€ ë§ ê²ƒ**")
    lines.append(f"   - í”„ë¡¬í”„íŠ¸ {len(prompts)}ê°œ? ì¤‘ìš”í•œ ê±´ í’ˆì§ˆ")
    lines.append(f"   - í‰ê·  {avg_length:.0f}ì? ì¤‘ìš”í•œ ê±´ ëª…í™•ì„±")
    lines.append("")
    lines.append("3. **ëª¨ë¸ê³¼ í•¨ê»˜ í•™ìŠµí•˜ê¸°**")
    lines.append("   - í”„ë¡¬í”„íŠ¸ë„ ì§„í™”í•´ì•¼ í•¨")
    lines.append("   - ì´ë²ˆ ì£¼ íŒ¨í„´ì´ ë‹¤ìŒ ì£¼ì—” ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ")
    lines.append("")

    return "\n".join(lines)


def deep_pattern_insights(prompts: List[Dict], commits: List[Dict]) -> str:
    """íŒ¨í„´ ê¹Šì´ ìˆëŠ” ë¶„ì„ (2000ì)"""
    lines = []
    lines.append("## ğŸ” Part 4: íŒ¨í„´ ì¸ì‚¬ì´íŠ¸ - ë‹¹ì‹ ì˜ ì‘ì—… ìŠ¤íƒ€ì¼")
    lines.append("")

    # ì†ŒìŠ¤ë³„ ë¶„í¬
    sources = Counter(p.get('source') for p in prompts)
    lines.append("### ì£¼ë ¥ ë„êµ¬ ë¶„ì„")
    lines.append("")

    for source, count in sources.most_common():
        pct = count / len(prompts) * 100
        lines.append(f"**{source}**: {count}ê°œ ({pct:.1f}%)")
        lines.append("")

        if source == "claude_code" and pct > 50:
            lines.append("Claude Codeê°€ ì£¼ë ¥ì…ë‹ˆë‹¤.")
            lines.append("íŠ¹ì§•: í”„ë¡œì íŠ¸ ê¸°ë°˜, íŒŒì¼ í¸ì§‘, í„°ë¯¸ë„ í†µí•©")
            lines.append("ê°•ì : ë§¥ë½ ìœ ì§€, ì—°ì† ì‘ì—…")
            lines.append("")

        elif source == "codex" and pct > 30:
            lines.append("Codex ì‚¬ìš©ì´ í™œë°œí•©ë‹ˆë‹¤.")
            lines.append("íŠ¹ì§•: í„°ë¯¸ë„ ì¤‘ì‹¬, ë¹ ë¥¸ ì‹¤í–‰")
            lines.append("ê°•ì : ì¦‰ê°ì  í”¼ë“œë°±, ì…¸ í†µí•©")
            lines.append("")

    # ì‹œê°„ íŒ¨í„´
    if commits:
        lines.append("### ì‹œê°„ íŒ¨í„´ ë¶„ì„")
        lines.append("")

        commit_times = [c['time'][:2] for c in commits]  # ì‹œê°„ë§Œ
        hour_dist = Counter(commit_times)

        lines.append("**ì»¤ë°‹ ì‹œê°„ëŒ€**:")
        for hour, count in sorted(hour_dist.items()):
            lines.append(f"- {hour}ì‹œ: {count}ê±´")
        lines.append("")

        # íŒ¨í„´ í•´ì„
        morning = sum(count for hour, count in hour_dist.items() if '06' <= hour < '12')
        afternoon = sum(count for hour, count in hour_dist.items() if '12' <= hour < '18')
        evening = sum(count for hour, count in hour_dist.items() if '18' <= hour < '24')
        night = sum(count for hour, count in hour_dist.items() if '00' <= hour < '06')

        lines.append("**ê·¼ë¬´ íŒ¨í„´**:")
        lines.append(f"- ì˜¤ì „ (06-12): {morning}ê±´")
        lines.append(f"- ì˜¤í›„ (12-18): {afternoon}ê±´")
        lines.append(f"- ì €ë… (18-24): {evening}ê±´")
        lines.append(f"- ì‹¬ì•¼ (00-06): {night}ê±´")
        lines.append("")

        if evening + night > morning + afternoon:
            lines.append("ğŸ¦‰ **Night Owl íŒ¨í„´**")
            lines.append("ì €ë…/ì‹¬ì•¼ ì‘ì—…ì´ ë§ìŠµë‹ˆë‹¤.")
            lines.append("")
            lines.append("**ì¥ì **: ë°©í•´ ì—†ëŠ” ì§‘ì¤‘ ì‹œê°„")
            lines.append("**ì£¼ì˜**: ìˆ˜ë©´ íŒ¨í„´ ì²´í¬ í•„ìš”")
            lines.append("")
        elif morning > afternoon + evening:
            lines.append("ğŸ¤ **Early Bird íŒ¨í„´**")
            lines.append("ì˜¤ì „ ì‘ì—…ì´ ì§‘ì¤‘ë©ë‹ˆë‹¤.")
            lines.append("")
            lines.append("**ì¥ì **: í•˜ë£¨ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‹œì‘")
            lines.append("**ì£¼ì˜**: ì˜¤í›„ ì—ë„ˆì§€ ê´€ë¦¬")
            lines.append("")

    # í”„ë¡¬í”„íŠ¸ íƒ€ì… ê· í˜•
    lines.append("### í”„ë¡¬í”„íŠ¸ íƒ€ì… ê· í˜•")
    lines.append("")

    question_count = sum(1 for p in prompts if '?' in p.get('content', ''))
    command_count = sum(1 for p in prompts if any(word in p.get('content', '') for word in ['í•´ì¤˜', 'ë§Œë“¤ì–´']))
    review_count = sum(1 for p in prompts if any(word in p.get('content', '') for word in ['í™•ì¸', 'ë¦¬ë·°', 'ì²´í¬']))

    total_typed = question_count + command_count + review_count
    if total_typed > 0:
        lines.append(f"- ì§ˆë¬¸í˜•: {question_count}ê°œ ({question_count/total_typed*100:.1f}%)")
        lines.append(f"- ì§€ì‹œí˜•: {command_count}ê°œ ({command_count/total_typed*100:.1f}%)")
        lines.append(f"- ê²€í† í˜•: {review_count}ê°œ ({review_count/total_typed*100:.1f}%)")
        lines.append("")

        if question_count > command_count + review_count:
            lines.append("ğŸ’¡ **íƒìƒ‰ ë‹¨ê³„**: ì§ˆë¬¸ì´ ë§ìŠµë‹ˆë‹¤")
            lines.append("ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš°ê³  ìˆê±°ë‚˜, ë¬¸ì œë¥¼ ì´í•´í•˜ëŠ” ë‹¨ê³„")
        elif command_count > question_count + review_count:
            lines.append("ğŸ”¨ **ì‹¤í–‰ ë‹¨ê³„**: ì§€ì‹œê°€ ë§ìŠµë‹ˆë‹¤")
            lines.append("êµ¬í˜„ì— ì§‘ì¤‘í•˜ëŠ” ë‹¨ê³„")
        elif review_count > 0:
            lines.append("ğŸ” **ê²€í†  ë‹¨ê³„**: í™•ì¸ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤")
            lines.append("í’ˆì§ˆ ê´€ë¦¬ì— ì‹ ê²½ ì“°ëŠ” ì¢‹ì€ ì‹ í˜¸")
        else:
            lines.append("âš–ï¸ **ê· í˜•**: íƒìƒ‰, ì‹¤í–‰, ê²€í† ê°€ ê· í˜•ì„ ì´ë£¹ë‹ˆë‹¤")

    lines.append("")

    return "\n".join(lines)


def deep_next_week_goals(data: Dict[str, Any]) -> str:
    """ë‹¤ìŒ ì£¼ SMART ëª©í‘œ (2000ì)"""
    lines = []
    lines.append("## ğŸ¯ Part 5: ë‹¤ìŒ ì£¼ SMART ëª©í‘œ + ì‹¤í–‰ ê³„íš")
    lines.append("")

    prompts = data.get('prompts', [])
    commits = data.get('commits', [])

    # Goal 1: ì»¤ë°‹ ë¶„ì‚°
    lines.append("### Goal 1: ì»¤ë°‹ ë¶„ì‚° (ë§¤ì¼ ì»¤ë°‹)")
    lines.append("")
    lines.append(f"**í˜„ì¬ ìƒíƒœ**: {len(commits)}ê°œ ì»¤ë°‹")
    lines.append("")

    commit_dates = len(set(c['date'] for c in commits))
    lines.append(f"**ë¶„ì‚°ë„**: {commit_dates}ì¼ / 7ì¼")
    lines.append("")

    lines.append("**SMART ëª©í‘œ**:")
    lines.append("- Specific: ë§¤ì¼ ìµœì†Œ 1ê°œ ì»¤ë°‹")
    lines.append("- Measurable: `git log --since='1 week ago' --format='%ad' --date=short | uniq | wc -l` >= 7")
    lines.append("- Achievable: ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°")
    lines.append("- Relevant: Surgical Changes ì›ì¹™ ê°•í™”")
    lines.append("- Time-bound: ë‹¤ìŒ ê¸ˆìš”ì¼ê¹Œì§€")
    lines.append("")

    lines.append("**ì‹¤í–‰ ê³„íš**:")
    lines.append("1. ì•„ì¹¨: ì–´ì œ ì‘ì—… ì»¤ë°‹ í™•ì¸")
    lines.append("2. ì‘ì—… ì‹œì‘ ì „: ì˜¤ëŠ˜ì˜ ì»¤ë°‹ ëª©í‘œ ì •í•˜ê¸°")
    lines.append("3. ì ì‹¬ í›„: ì˜¤ì „ ì‘ì—… ì»¤ë°‹")
    lines.append("4. í‡´ê·¼ ì „: ì˜¤í›„ ì‘ì—… ì»¤ë°‹")
    lines.append("")

    lines.append("**ì˜ˆìƒ ì¥ì• ë¬¼**:")
    lines.append("- \"ì•„ì§ ì™„ì„± ì•ˆ ëëŠ”ë° ì»¤ë°‹?\"")
    lines.append("  â†’ WIP (Work In Progress) ì»¤ë°‹ OK")
    lines.append("  â†’ ë‚˜ì¤‘ì— rebaseë¡œ ì •ë¦¬ ê°€ëŠ¥")
    lines.append("")

    # Goal 2: í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ
    avg_length = sum(len(p.get('content', '')) for p in prompts) / max(len(prompts), 1)

    lines.append("### Goal 2: í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ 70ì  ì´ìƒ")
    lines.append("")
    lines.append(f"**í˜„ì¬**: í‰ê·  {avg_length:.1f}ì")
    lines.append("")

    lines.append("**í’ˆì§ˆ ê¸°ì¤€**:")
    lines.append("1. ê¸¸ì´: 30-200ì (30ì )")
    lines.append("2. ë§¥ë½ ì œê³µ: 'ì™œ', 'ìœ„í•´' í¬í•¨ (20ì )")
    lines.append("3. êµ¬ì²´ì : 10ë‹¨ì–´ ì´ìƒ (20ì )")
    lines.append("4. ê²€ì¦ ê°€ëŠ¥: ëª©í‘œ ëª…ì‹œ (30ì )")
    lines.append("")

    lines.append("**ì‹¤í–‰ ê³„íš**:")
    lines.append("1. í”„ë¡¬í”„íŠ¸ ì‘ì„± ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
    lines.append("   - [ ] ë§¥ë½ì„ ì„¤ëª…í–ˆëŠ”ê°€?")
    lines.append("   - [ ] ëª©í‘œê°€ ëª…í™•í•œê°€?")
    lines.append("   - [ ] ê²€ì¦ ë°©ë²•ì´ ìˆëŠ”ê°€?")
    lines.append("")
    lines.append("2. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©:")
    lines.append("   \"{ì‘ì—…} + {ì´ìœ } + {ì„±ê³µ ì¡°ê±´}\"")
    lines.append("")
    lines.append("**ì˜ˆì‹œ**:")
    lines.append("Before: \"í…ŒìŠ¤íŠ¸ ë§Œë“¤ì–´ì¤˜\"")
    lines.append("After: \"ë¡œê·¸ì¸ API í…ŒìŠ¤íŠ¸ ë§Œë“¤ì–´ì¤˜. ì¸ì¦ ë²„ê·¸ê°€ ê³„ì† ë‚˜ì„œ. ì„±ê³µ/ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ê° 3ê°œì”©\"")
    lines.append("")

    # Goal 3: Karpathy ì›ì¹™
    lines.append("### Goal 3: Karpathy ì¢…í•© ì ìˆ˜ 60ì  ì´ìƒ")
    lines.append("")
    lines.append("**í˜„ì¬**: 37ì  (Think 32, Simplicity 0, Surgical 100, Goal 17)")
    lines.append("")

    lines.append("**ì§‘ì¤‘ ì˜ì—­**: Simplicity First (í˜„ì¬ 0ì )")
    lines.append("")

    lines.append("**ì‹¤í–‰ ê³„íš**:")
    lines.append("1. ë§¤ì£¼ 1ê°œ íŒŒì¼ ë¦¬íŒ©í† ë§")
    lines.append("2. ë¦¬íŒ©í† ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸:")
    lines.append("   - [ ] ì¤‘ë³µ ì½”ë“œ ì œê±°")
    lines.append("   - [ ] í•¨ìˆ˜ ë¶„ë¦¬ (50ì¤„ ì´í•˜)")
    lines.append("   - [ ] ë³€ìˆ˜ëª… ëª…í™•í™”")
    lines.append("   - [ ] ë¶ˆí•„ìš”í•œ ì£¼ì„ ì œê±°")
    lines.append("")

    lines.append("**íƒ€ê²Ÿ**:")
    lines.append("ê°€ì¥ ê¸´ íŒŒì¼/í•¨ìˆ˜ ì°¾ê¸°:")
    lines.append("```bash")
    lines.append("find . -name '*.py' -exec wc -l {} \\; | sort -rn | head -5")
    lines.append("```")
    lines.append("")

    return "\n".join(lines)


def deep_study_loop_section(prompts: List[Dict], workdir: str) -> str:
    """
    ML 16ì£¼ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì§„ë„ Loop ì„¹ì…˜ (ì£¼ê°„ íšŒê³  ì „ìš©).
    study_trackerë¥¼ í˜¸ì¶œí•´ ì´ë²ˆ ì£¼ í•™ìŠµ ì¦ê±°ë¥¼ ë¶„ì„í•˜ê³ 
    ë‹¤ìŒ ì£¼ í•™ìŠµ ê³„íšì„ ì•ˆë‚´í•œë‹¤.
    """
    lines = []
    lines.append("## ğŸ“š Part 6: ML í•™ìŠµ ì§„ë„ Loop")
    lines.append("")
    lines.append("> *\"API wrapper íƒˆì¶œ\" 16ì£¼ ì»¤ë¦¬í˜ëŸ¼ â€” ë§¤ì£¼ ìë™ ì§„ë„ ì²´í¬*")
    lines.append("")

    try:
        sys.path.insert(0, str(Path(__file__).parent))
        from study_tracker import (
            load_study_plan,
            get_current_week_info,
            detect_study_prompts,
            build_study_report,
            format_report_markdown,
            collect_recent_prompts,
            MIN_STUDY_PROMPTS_WEEKLY,
        )

        plan = load_study_plan()
        if not plan:
            lines.append("âš ï¸ `config/study_plan.json`ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            lines.append("ì»¤ë¦¬í˜ëŸ¼ ì‹œì‘ ë‚ ì§œì™€ ê³„íšì„ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return "\n".join(lines)

        week_info = get_current_week_info(plan)
        status = week_info.get("status", "unknown")

        if status == "not_started":
            lines.append(f"â³ {week_info.get('message', '')}")
            lines.append("")
            lines.append("ì»¤ë¦¬í˜ëŸ¼ ì‹œì‘ ì „! ì´ë²ˆ ì£¼ì— ë¯¸ë¦¬ ì¤€ë¹„í•˜ì„¸ìš”:")
            lines.append("1. Vaswani 2017 ë…¼ë¬¸ ë‹¤ìš´ë¡œë“œ")
            lines.append("2. PyTorch ì„¤ì¹˜ í™•ì¸")
            lines.append("3. í•˜ë£¨ 3ì‹œê°„ ë¸”ë¡ ìº˜ë¦°ë”ì— ì¡ê¸°")
            return "\n".join(lines)

        if status == "completed":
            lines.append(f"ğŸ‰ {week_info.get('message', '')}")
            lines.append("")
            lines.append("16ì£¼ ì™„ì£¼ ë‹¬ì„±! ì´ì œ ì§„ì§œ 'API wrapper íƒˆì¶œ' ì™„ì„±.")
            return "\n".join(lines)

        # í™œì„± ì£¼ì°¨ â€” ìƒì„¸ ë¶„ì„
        current_week = week_info["week"]
        topic = week_info["topic"]
        phase_name = week_info["phase_name"]
        keywords = week_info["keywords"]

        # ì´ë²ˆ ì£¼ í”„ë¡¬í”„íŠ¸ì—ì„œ í•™ìŠµ ì¦ê±° íƒì§€
        recent = collect_recent_prompts(8, workdir)  # 8ì¼ (ì£¼ê°„ + ì—¬ìœ )
        matched, high_quality = detect_study_prompts(recent, keywords, topic)

        lines.append(f"### Week {current_week}: {topic}")
        lines.append(f"**Phase {week_info['phase']}**: {phase_name}")
        lines.append(f"**ë…¼ë¬¸**: {week_info['paper']}")
        lines.append(f"**ëª©í‘œ**: {week_info['goal']}")
        lines.append(f"**ì‚°ì¶œë¬¼**: {week_info['deliverable']}")
        lines.append(f"**ê¸°ê°„**: {week_info['week_start']} ~ {week_info['week_end']}")
        lines.append("")

        # ì§„ë„ íŒì •
        threshold = MIN_STUDY_PROMPTS_WEEKLY
        match_count = len(matched)
        hq_count = len(high_quality)

        if match_count == 0:
            verdict = "ğŸ”´ FAIL â€” ì´ë²ˆ ì£¼ í•™ìŠµ í”ì  ì—†ìŒ"
            verdict_detail = (
                "Codexì—ê²Œ ë…¼ë¬¸ ë‚´ìš©ì„ ë‹¨ í•œ ë²ˆë„ ë¬¼ì–´ë³´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
                "ê³µë¶€í•œ ê²ƒê³¼ ì•ˆ í•œ ê²ƒì€ ìˆ«ìê°€ ë§í•´ì¤ë‹ˆë‹¤."
            )
        elif match_count < threshold // 2:
            pct = int(match_count / threshold * 100)
            verdict = f"ğŸŸ  WEAK â€” ëª©í‘œì˜ {pct}% ë‹¬ì„±"
            verdict_detail = (
                f"í•™ìŠµ í”„ë¡¬í”„íŠ¸ {match_count}ê°œ íƒì§€ (ëª©í‘œ: {threshold}ê°œ).\n"
                "ì¡°ê¸ˆ ë” ê¹Šì´ íŒŒê³ ë“¤ì–´ì•¼ í•©ë‹ˆë‹¤."
            )
        elif match_count < threshold:
            verdict = f"ğŸŸ¡ PARTIAL â€” {match_count}/{threshold}ê°œ"
            verdict_detail = "ì ˆë°˜ ì´ìƒ ë‹¬ì„±! ë§ˆì§€ë§‰ ìŠ¤í¼íŠ¸ í•„ìš”."
        else:
            verdict = f"ğŸŸ¢ ACHIEVED â€” {match_count}ê°œ (ëª©í‘œ {threshold}ê°œ ì´ˆê³¼)"
            verdict_detail = "ì´ë²ˆ ì£¼ í•™ìŠµ ëª©í‘œ ì™„ì „ ë‹¬ì„±! í›Œë¥­í•©ë‹ˆë‹¤."

        lines.append(f"**ì´ë²ˆ ì£¼ ì§„ë„**: {verdict}")
        lines.append(f"{verdict_detail}")
        lines.append("")
        lines.append(f"- íƒì§€ëœ í•™ìŠµ í”„ë¡¬í”„íŠ¸: {match_count}ê°œ")
        lines.append(f"- ê³ í’ˆì§ˆ í•™ìŠµ í”„ë¡¬í”„íŠ¸ (í‚¤ì›Œë“œ 2ê°œ+ ë§¤ì¹­): {hq_count}ê°œ")
        lines.append("")

        # í•™ìŠµ ì¦ê±° ìƒ˜í”Œ
        if matched:
            lines.append("**ì´ë²ˆ ì£¼ í•™ìŠµ í”ì  (ìƒìœ„ 5ê°œ)**:")
            for p in matched[:5]:
                kws = ", ".join(p.get("_matched_keywords", [])[:3])
                content = (p.get("content", "") or "")[:100]
                src = p.get("source", "?")
                t = p.get("time", "")
                lines.append(f'- `[{src} {t}]` "{content}" â†’ `{kws}`')
            lines.append("")
        else:
            lines.append("**í•™ìŠµ í”ì **: ì—†ìŒ")
            lines.append("")

        # ë‹¤ìŒ ì£¼ ì˜ˆê³  ë° í˜„ì¬ ì£¼ ë§ˆë¬´ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸
        lines.append("### ì´ë²ˆ ì£¼ ë§ˆë¬´ë¦¬ ì²´í¬ë¦¬ìŠ¤íŠ¸")
        lines.append("")
        deliverable = week_info["deliverable"]
        lines.append(f"- [ ] ì‚°ì¶œë¬¼ ì™„ì„±: **{deliverable}**")
        lines.append(f"- [ ] ë…¼ë¬¸/ìë£Œ ìµœì†Œ 3ì‹œê°„ ì½ê¸°")
        lines.append(f"- [ ] ìˆ˜ì‹/ê°œë… ì†ìœ¼ë¡œ ì •ë¦¬")
        lines.append(f"- [ ] Codexì—ê²Œ í•µì‹¬ ì§ˆë¬¸ {threshold}ê°œ ì´ìƒ ë˜ì§€ê¸°")
        lines.append("")

        # ë‹¤ìŒ ì£¼ ì˜ˆê³ 
        next_week_num = current_week + 1
        if next_week_num <= plan.get("total_weeks", 16):
            next_week_info = get_current_week_info(plan, override_week=next_week_num)
            if next_week_info.get("status") in ("active", "unknown"):
                lines.append(f"### ë‹¤ìŒ ì£¼ ì˜ˆê³ : Week {next_week_num}")
                lines.append(f"**ì£¼ì œ**: {next_week_info.get('topic', '?')}")
                lines.append(f"**ë…¼ë¬¸**: {next_week_info.get('paper', '?')}")
                lines.append(f"**ëª©í‘œ**: {next_week_info.get('goal', '?')}")
                lines.append("")
                # ë‹¤ìŒ ì£¼ ì¤€ë¹„ ì¶”ì²œ ì§ˆë¬¸ 1ê°œ
                next_kws = next_week_info.get("keywords", [])
                if next_kws:
                    lines.append("**ë¯¸ë¦¬ ìƒê°í•´ë³¼ ì§ˆë¬¸**:")
                    lines.append(f'"{next_week_info.get("goal", "ë‹¤ìŒ ì£¼ì œë¥¼ ë¯¸ë¦¬ ì¡°ì‚¬í•´ë³´ì„¸ìš”")}"')
                lines.append("")

        # ì „ì²´ ì§„ë„ ë°”
        total_weeks = plan.get("total_weeks", 16)
        done_pct = int((current_week - 1) / total_weeks * 100)
        bar_filled = int(done_pct / 5)
        bar = "â–ˆ" * bar_filled + "â–‘" * (20 - bar_filled)
        lines.append(f"**ì „ì²´ ì§„ë„**: [{bar}] Week {current_week}/{total_weeks} ({done_pct}%)")
        lines.append("")

        if match_count == 0:
            lines.append("---")
            lines.append("âš¡ **ê²½ê³ **: ì´ë²ˆ ì£¼ í•™ìŠµ ê¸°ë¡ì´ ì „í˜€ ì—†ìŠµë‹ˆë‹¤.")
            lines.append("Codexì—ì„œ ë…¼ë¬¸ ê´€ë ¨ ì§ˆë¬¸ì„ ì‹œì‘í•˜ì„¸ìš”. ì§€ê¸ˆ ë°”ë¡œ.")
            lines.append("")
            lines.append("**ì²« ì§ˆë¬¸ ì˜ˆì‹œ**:")
            next_q = week_info.get("goal", topic)
            lines.append(f'> "{next_q}"')

    except ImportError as e:
        lines.append(f"âš ï¸ study_tracker import ì‹¤íŒ¨: {e}")
    except Exception as e:
        lines.append(f"âš ï¸ í•™ìŠµ ì§„ë„ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

    return "\n".join(lines)


def run(input_data: dict, context: dict) -> dict:
    """ê¹Šì´ ìˆëŠ” ì£¼ê°„ íšŒê³  ì‹¤í–‰"""
    days_back = input_data.get("days_back", 7)
    workdir = context.get("workdir", ".")

    print("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì¤‘...", file=sys.stderr)

    # í”„ë¡¬í”„íŠ¸ ìˆ˜ì§‘ (ìµœê·¼ Nì¼ + ì´ì „ Nì¼ ìœˆë„ìš°)
    prompts, prev_prompts, collection_meta = collect_prompt_windows(days_back, workdir)

    # Git ì»¤ë°‹
    commits = collect_git_commits(days_back, workdir)

    print("ğŸ§  ê¹Šì´ ìˆëŠ” ë¶„ì„ ì¤‘...", file=sys.stderr)

    # ë°ì´í„°
    data = {
        "prompts": prompts,
        "commits": commits,
        "prev_prompts": prev_prompts
    }

    # ë§ˆí¬ë‹¤ìš´ ìƒì„±
    lines = []
    lines.append(f"# ì£¼ê°„ íšŒê³  (Week {datetime.now().strftime('%W')}, {datetime.now().strftime('%Y-%m-%d')})")
    lines.append("")
    lines.append("> **Karpathy ì›ì¹™ + Bitter Lesson + 1ë§Œì í”¼ë“œë°±**")
    lines.append("")

    # Part 1: ìš”ì•½
    lines.append("## ğŸ“Š Part 1: Executive Summary")
    lines.append("")
    lines.append(f"**ê¸°ê°„**: ìµœê·¼ {days_back}ì¼")
    lines.append(f"**í”„ë¡¬í”„íŠ¸**: {len(prompts)}ê°œ")
    lines.append(f"**ì»¤ë°‹**: {len(commits)}ê°œ")
    lines.append(f"**ë¹„êµêµ°(ì´ì „ {days_back}ì¼)**: {len(prev_prompts)}ê°œ")
    if collection_meta.get("collector_success"):
        lines.append("**ìˆ˜ì§‘ ë°©ì‹**: universal_prompt_collector ìµœì‹  ì¬ìˆ˜ì§‘")
    else:
        lines.append("**ìˆ˜ì§‘ ë°©ì‹**: fallback íŒŒì¼ ë¡œë“œ")
    lines.append("")

    sources = Counter(p.get('source') for p in prompts)
    lines.append("**í”„ë¡¬í”„íŠ¸ ì†ŒìŠ¤**:")
    prompt_count = max(len(prompts), 1)
    for source, count in sources.most_common():
        lines.append(f"- {source}: {count}ê°œ ({count/prompt_count*100:.1f}%)")
    lines.append("")

    date_dist = Counter(p.get("date", "unknown") for p in prompts)
    lines.append("**ë‚ ì§œ ë¶„í¬**:")
    for date_label, count in sorted(date_dist.items(), key=lambda x: x[0], reverse=True):
        lines.append(f"- {date_label}: {count}ê°œ")
    lines.append("")

    lines.append("---")
    lines.append("")

    # Part 2-5: ê¹Šì´ ìˆëŠ” ë¶„ì„
    lines.append(deep_karpathy_analysis(prompts, commits))
    lines.append("")
    lines.append(deep_bitter_lesson_analysis(prompts, prev_prompts))
    lines.append("")
    lines.append(deep_pattern_insights(prompts, commits))
    lines.append("")
    lines.append(deep_next_week_goals(data))
    lines.append("")

    # Part 6: ML í•™ìŠµ ì§„ë„ Loop
    lines.append(deep_study_loop_section(prompts, workdir))
    lines.append("")

    # Part 7: ë©”íƒ€ íšŒê³ 
    lines.append("## ğŸ”„ Part 7: ë©”íƒ€ íšŒê³  - ì´ íšŒê³ ì— ëŒ€í•œ íšŒê³ ")
    lines.append("")
    lines.append("**ì´ íšŒê³ ëŠ”**:")
    lines.append("- Karpathy 4ê°€ì§€ ì›ì¹™ ì ìš© âœ…")
    lines.append("- Bitter Lesson ê¸°ë°˜ ë¶„ì„ âœ…")
    lines.append(f"- {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸ ì „ìˆ˜ ì¡°ì‚¬ âœ…")
    lines.append("- êµ¬ì²´ì  ì‚¬ë¡€ì™€ í”¼ë“œë°± âœ…")
    lines.append("- ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ í”Œëœ âœ…")
    lines.append("- ML í•™ìŠµ ì§„ë„ ìë™ ì²´í¬ âœ…")
    lines.append("")

    markdown = "\n".join(lines)

    # íŒŒì¼ ì €ì¥
    output_file = Path(workdir) / f"deep_weekly_retrospective_{datetime.now().strftime('%Y_week%W')}.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown)

    return {
        "success": True,
        "output_file": str(output_file),
        "char_count": len(markdown),
        "word_count": len(markdown.split()),
        "summary": {
            "prompts": len(prompts),
            "commits": len(commits),
            "sections": 7,
            "prev_prompts": len(prev_prompts),
            "sources": dict(sources),
            "collector_success": bool(collection_meta.get("collector_success")),
        },
        "collection_meta": collection_meta,
    }


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--tool-spec-json", action="store_true")
    parser.add_argument("--tool-input-json", type=str)
    parser.add_argument("--tool-context-json", type=str)

    args = parser.parse_args()

    if args.tool_spec_json:
        print(json.dumps(TOOL_SPEC, ensure_ascii=False, indent=2))
        sys.exit(0)

    input_data = json.loads(args.tool_input_json) if args.tool_input_json else {}
    context = json.loads(args.tool_context_json) if args.tool_context_json else {}

    result = run(input_data, context)
    print(json.dumps(result, ensure_ascii=False, indent=2))
